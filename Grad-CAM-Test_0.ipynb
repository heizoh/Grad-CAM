{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd020caf1043d001e22867572d43ddb31c789313b650669922c128ec32dc494996c",
   "display_name": "Python 3.7.9 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Layer,Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = sys.argv[1]\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.common.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def _compute_gradients(tensor, var_list):\n",
    "    grads = tf.gradients(tensor, var_list)\n",
    "    return [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(var_list, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_model : model\n",
    "        評価するKerasモデル\n",
    "    image : tuple等\n",
    "        入力画像(枚数, 縦, 横, チャンネル)\n",
    "    category_index : int\n",
    "        入力画像の分類クラス\n",
    "    layer_name : str\n",
    "        最後のconv層の後のactivation層のレイヤー名.\n",
    "        最後のconv層でactivationを指定していればconv層のレイヤー名.\n",
    "        batch_normalizationを使う際などのようなconv層でactivationを指定していない場合は、\n",
    "        そのあとのactivation層のレイヤー名.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    cam : tuple\n",
    "        Grad-Camの画像\n",
    "    heatmap : tuple\n",
    "        ヒートマップ画像\n",
    "    '''\n",
    "\n",
    "    # ----- 1. 入力画像の予測クラスを計算 -----\n",
    "\n",
    "    # 入力のcategory_indexが予想クラス\n",
    "\n",
    "    # ----- 2. 予測クラスのLossを計算 -----\n",
    "    # 分類クラス数\n",
    "    nb_classes = 2\n",
    "\n",
    "    # 入力データxのcategory_indexで指定したインデックス以外を0にする処理の定義\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    x = input_model.output\n",
    "    x = Lambda(target_layer, output_shape=target_category_loss_output_shape)(x)\n",
    "    model = tf.keras.models.Model([input_model.inputs], [input_model.get_layer(layer_name).output, input_model.output])\n",
    "    #model.summary()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    " \n",
    "        # 引数のinput_modelの出力層の後にtarget_layerレイヤーを追加\n",
    "        # modelのpredictをすると予測クラス以外の値は0になる\n",
    "        # 予測クラス以外の値は0なのでsumをとって予測クラスの値のみ抽出\n",
    "        conv_output, preds = model(image)\n",
    "        class_idx = np.argmax(preds[0])\n",
    "        loss = preds[:, class_idx]\n",
    "        # 引数のlayer_nameのレイヤー(最後のconv層)のoutputを取得する\n",
    "        # conv_output = [l for l in model.layers if l.name == layer_name][0].output\n",
    "        # var_list = [conv_output]\n",
    "\n",
    "    # ----- 3. 予測クラスのLossから最後のconv層への逆伝搬(勾配)を計算 -----\n",
    "\n",
    "    # 予想クラスの値から最後のconv層までの勾配を計算する関数を定義\n",
    "    # 定義した関数の\n",
    "    # 入力 : [判定したい画像.shape=(1, 224, 224, 3)]、\n",
    "    # 出力 : [最後のconv層の出力値.shape=(1, 14, 14, 512), 予想クラスの値から最後のconv層までの勾配.shape=(1, 14, 14, 512)]\n",
    "    output =conv_output[0]\n",
    "    grads = tape.gradient(loss,conv_output)[0]\n",
    "\n",
    "    #gradient_function = K.function([model.input], [conv_output, grads[0]])\n",
    "    gate_f = tf.cast(output > 0,'float32')\n",
    "    gate_r = tf.cast(grads > 0,'float32')\n",
    "\n",
    "    # 定義した勾配計算用の関数で計算し、データの次元を整形\n",
    "    # 整形後\n",
    "    # output.shape=(14, 14, 512), grad_val.shape=(14, 14, 512)\n",
    "    #output, grads_val = gradient_function([image])\n",
    "    #output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "    guided_grads = gate_f * gate_r * grads\n",
    "\n",
    "    # ----- 4. 最後のconv層のチャンネル毎に勾配を平均を計算して、各チャンネルの重要度(重み)とする -----\n",
    "\n",
    "    # weights.shape=(512, )\n",
    "    # cam.shape=(14, 14)\n",
    "    # ※疑問点1：camの初期化はzerosでなくて良いのか?\n",
    "    weights = np.mean(guided_grads, axis = (0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "    #cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "    #cam = np.zeros(output.shape[0 : 2], dtype = np.float32)    # 私の自作モデルではこちらを使用\n",
    "\n",
    "    # ----- 5. 最後のconv層の順伝搬の出力にチャンネル毎の重みをかけて、足し合わせて、ReLUを通す -----\n",
    "\n",
    "    # 最後のconv層の順伝搬の出力にチャンネル毎の重みをかけて、足し合わせ\n",
    "    #for i, w in enumerate(weights):\n",
    "    #    cam += w * output[:, :, i]\n",
    "\n",
    "    # 入力画像のサイズにリサイズ(14, 14) → (224, 224)\n",
    "    cam = cv2.resize(cam, (210, 210))\n",
    "    # 負の値を0に置換。処理としてはReLUと同じ。\n",
    "    cam = np.maximum(cam, 0)\n",
    "    # 値を0~1に正規化。\n",
    "    # ※疑問2 : (cam - np.min(cam))/(np.max(cam) - np.min(cam))でなくて良いのか?\n",
    "    heatmap = cam / np.max(cam)\n",
    "    #heatmap = (cam - np.min(cam))/(np.max(cam) - np.min(cam))    # 私の自作モデルではこちらを使用\n",
    "\n",
    "    # ----- 6. 入力画像とheatmapをかける -----\n",
    "\n",
    "    # 入力画像imageの値を0~255に正規化. image.shape=(1, 224, 224, 3) → (224, 224, 3)\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    # ※疑問3 : np.uint8(image / np.max(image))でなくても良いのか?\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    # heatmapの値を0~255にしてカラーマップ化(3チャンネル化)\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    # 入力画像とheatmapの足し合わせ\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    # 値を0~255に正規化\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(include_top=False, weights='imagenet',input_shape=(210,210,3))\n",
    "#plot_model(base_model,show_shapes=True,to_file='graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(210,210,3))\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(1024,activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(2,activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input,outputs=x)\n",
    "#plot_model(model,show_shapes=True,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "    if layer.name.startswith('batch_normalization'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.load('washer_OK.npy')\n",
    "x1 = np.load('washer_NG_ponch.npy')\n",
    "x2 = np.load('washer_NG_Scratch.npy')\n",
    "X = np.concatenate([x0[:172],x1,x2],0)\n",
    "X = X.reshape(344,210,210,1)\n",
    "X = np.tile(X,(1,1,1,3))\n",
    "y = np.concatenate([np.zeros((172)),np.ones((172))],0)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 2.9276 - accuracy: 0.5068\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 1.0226 - accuracy: 0.6918\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.4073 - accuracy: 0.7911\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.2641 - accuracy: 0.8699\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.1015 - accuracy: 0.9692\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.1460 - accuracy: 0.9178\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.0850 - accuracy: 0.9863\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.2048 - accuracy: 0.9452\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.2242 - accuracy: 0.9110\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.1494 - accuracy: 0.9692\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.1316 - accuracy: 0.9760\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.0964 - accuracy: 0.9726\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0864 - accuracy: 0.9795\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0307 - accuracy: 0.9966\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0384 - accuracy: 0.9863\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0494 - accuracy: 0.9966\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.0453 - accuracy: 0.9863\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0990 - accuracy: 0.9726\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.1552 - accuracy: 0.9418\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.1192 - accuracy: 0.9555\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0282 - accuracy: 0.9932\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.0408 - accuracy: 0.9829\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.0878 - accuracy: 0.9726\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.1046 - accuracy: 0.9555\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.0568 - accuracy: 0.9795\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.0529 - accuracy: 0.9897\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.0613 - accuracy: 0.9829\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.1378 - accuracy: 0.9486\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.0701 - accuracy: 0.9795\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 1s 200ms/step - loss: 0.1000 - accuracy: 0.9692\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,batch_size=96,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['input_1', 'conv2d', 'batch_normalization', 'activation', 'conv2d_1',\n 'batch_normalization_1', 'activation_1', 'conv2d_2', 'batch_normalization_2',\n 'activation_2', 'max_pooling2d', 'conv2d_3', 'batch_normalization_3',\n 'activation_3', 'conv2d_4', 'batch_normalization_4', 'activation_4',\n 'max_pooling2d_1', 'conv2d_8', 'batch_normalization_8', 'activation_8',\n 'conv2d_6', 'conv2d_9', 'batch_normalization_6', 'batch_normalization_9',\n 'activation_6', 'activation_9', 'average_pooling2d', 'conv2d_5', 'conv2d_7',\n 'conv2d_10', 'conv2d_11', 'batch_normalization_5', 'batch_normalization_7',\n 'batch_normalization_10', 'batch_normalization_11', 'activation_5',\n 'activation_7', 'activation_10', 'activation_11', 'mixed0', 'conv2d_15',\n 'batch_normalization_15', 'activation_15', 'conv2d_13', 'conv2d_16',\n 'batch_normalization_13', 'batch_normalization_16', 'activation_13',\n 'activation_16', 'average_pooling2d_1', 'conv2d_12', 'conv2d_14', 'conv2d_17',\n 'conv2d_18', 'batch_normalization_12', 'batch_normalization_14',\n 'batch_normalization_17', 'batch_normalization_18', 'activation_12',\n 'activation_14', 'activation_17', 'activation_18', 'mixed1', 'conv2d_22',\n 'batch_normalization_22', 'activation_22', 'conv2d_20', 'conv2d_23',\n 'batch_normalization_20', 'batch_normalization_23', 'activation_20',\n 'activation_23', 'average_pooling2d_2', 'conv2d_19', 'conv2d_21', 'conv2d_24',\n 'conv2d_25', 'batch_normalization_19', 'batch_normalization_21',\n 'batch_normalization_24', 'batch_normalization_25', 'activation_19',\n 'activation_21', 'activation_24', 'activation_25', 'mixed2', 'conv2d_27',\n 'batch_normalization_27', 'activation_27', 'conv2d_28',\n 'batch_normalization_28', 'activation_28', 'conv2d_26', 'conv2d_29',\n 'batch_normalization_26', 'batch_normalization_29', 'activation_26',\n 'activation_29', 'max_pooling2d_2', 'mixed3', 'conv2d_34',\n 'batch_normalization_34', 'activation_34', 'conv2d_35',\n 'batch_normalization_35', 'activation_35', 'conv2d_31', 'conv2d_36',\n 'batch_normalization_31', 'batch_normalization_36', 'activation_31',\n 'activation_36', 'conv2d_32', 'conv2d_37', 'batch_normalization_32',\n 'batch_normalization_37', 'activation_32', 'activation_37',\n 'average_pooling2d_3', 'conv2d_30', 'conv2d_33', 'conv2d_38', 'conv2d_39',\n 'batch_normalization_30', 'batch_normalization_33', 'batch_normalization_38',\n 'batch_normalization_39', 'activation_30', 'activation_33', 'activation_38',\n 'activation_39', 'mixed4', 'conv2d_44', 'batch_normalization_44',\n 'activation_44', 'conv2d_45', 'batch_normalization_45', 'activation_45',\n 'conv2d_41', 'conv2d_46', 'batch_normalization_41', 'batch_normalization_46',\n 'activation_41', 'activation_46', 'conv2d_42', 'conv2d_47',\n 'batch_normalization_42', 'batch_normalization_47', 'activation_42',\n 'activation_47', 'average_pooling2d_4', 'conv2d_40', 'conv2d_43', 'conv2d_48',\n 'conv2d_49', 'batch_normalization_40', 'batch_normalization_43',\n 'batch_normalization_48', 'batch_normalization_49', 'activation_40',\n 'activation_43', 'activation_48', 'activation_49', 'mixed5', 'conv2d_54',\n 'batch_normalization_54', 'activation_54', 'conv2d_55',\n 'batch_normalization_55', 'activation_55', 'conv2d_51', 'conv2d_56',\n 'batch_normalization_51', 'batch_normalization_56', 'activation_51',\n 'activation_56', 'conv2d_52', 'conv2d_57', 'batch_normalization_52',\n 'batch_normalization_57', 'activation_52', 'activation_57',\n 'average_pooling2d_5', 'conv2d_50', 'conv2d_53', 'conv2d_58', 'conv2d_59',\n 'batch_normalization_50', 'batch_normalization_53', 'batch_normalization_58',\n 'batch_normalization_59', 'activation_50', 'activation_53', 'activation_58',\n 'activation_59', 'mixed6', 'conv2d_64', 'batch_normalization_64',\n 'activation_64', 'conv2d_65', 'batch_normalization_65', 'activation_65',\n 'conv2d_61', 'conv2d_66', 'batch_normalization_61', 'batch_normalization_66',\n 'activation_61', 'activation_66', 'conv2d_62', 'conv2d_67',\n 'batch_normalization_62', 'batch_normalization_67', 'activation_62',\n 'activation_67', 'average_pooling2d_6', 'conv2d_60', 'conv2d_63', 'conv2d_68',\n 'conv2d_69', 'batch_normalization_60', 'batch_normalization_63',\n 'batch_normalization_68', 'batch_normalization_69', 'activation_60',\n 'activation_63', 'activation_68', 'activation_69', 'mixed7', 'conv2d_72',\n 'batch_normalization_72', 'activation_72', 'conv2d_73',\n 'batch_normalization_73', 'activation_73', 'conv2d_70', 'conv2d_74',\n 'batch_normalization_70', 'batch_normalization_74', 'activation_70',\n 'activation_74', 'conv2d_71', 'conv2d_75', 'batch_normalization_71',\n 'batch_normalization_75', 'activation_71', 'activation_75', 'max_pooling2d_3',\n 'mixed8', 'conv2d_80', 'batch_normalization_80', 'activation_80', 'conv2d_77',\n 'conv2d_81', 'batch_normalization_77', 'batch_normalization_81',\n 'activation_77', 'activation_81', 'conv2d_78', 'conv2d_79', 'conv2d_82',\n 'conv2d_83', 'average_pooling2d_7', 'conv2d_76', 'batch_normalization_78',\n 'batch_normalization_79', 'batch_normalization_82', 'batch_normalization_83',\n 'conv2d_84', 'batch_normalization_76', 'activation_78', 'activation_79',\n 'activation_82', 'activation_83', 'batch_normalization_84', 'activation_76',\n 'mixed9_0', 'concatenate', 'activation_84', 'mixed9', 'conv2d_89',\n 'batch_normalization_89', 'activation_89', 'conv2d_86', 'conv2d_90',\n 'batch_normalization_86', 'batch_normalization_90', 'activation_86',\n 'activation_90', 'conv2d_87', 'conv2d_88', 'conv2d_91', 'conv2d_92',\n 'average_pooling2d_8', 'conv2d_85', 'batch_normalization_87',\n 'batch_normalization_88', 'batch_normalization_91', 'batch_normalization_92',\n 'conv2d_93', 'batch_normalization_85', 'activation_87', 'activation_88',\n 'activation_91', 'activation_92', 'batch_normalization_93', 'activation_85',\n 'mixed9_1', 'concatenate_1', 'activation_93', 'mixed10',\n 'global_average_pooling2d', 'dense', 'dropout', 'dense_1']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "names = [l.name for l in model.layers]\n",
    "pprint.pprint(names,compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_test.shape[0]):\n",
    "    pre_inp = X_test[i,:,:,:].reshape(-1,210,210,3)\n",
    "    pred = model.predict(pre_inp)\n",
    "    pred_Class = np.argmax(pred)\n",
    "    cam,hm = grad_cam(model,pre_inp,pred_Class,'mixed10')\n",
    "    cv2.imwrite('hm_'+ str(i) +'.png', cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}