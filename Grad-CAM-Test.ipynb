{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd020caf1043d001e22867572d43ddb31c789313b650669922c128ec32dc494996c",
   "display_name": "Python 3.7.9 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Layer,Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = sys.argv[1]\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.common.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def _compute_gradients(tensor, var_list):\n",
    "    grads = tf.gradients(tensor, var_list)\n",
    "    return [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(var_list, grads)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_model : model\n",
    "        評価するKerasモデル\n",
    "    image : tuple等\n",
    "        入力画像(枚数, 縦, 横, チャンネル)\n",
    "    category_index : int\n",
    "        入力画像の分類クラス\n",
    "    layer_name : str\n",
    "        最後のconv層の後のactivation層のレイヤー名.\n",
    "        最後のconv層でactivationを指定していればconv層のレイヤー名.\n",
    "        batch_normalizationを使う際などのようなconv層でactivationを指定していない場合は、\n",
    "        そのあとのactivation層のレイヤー名.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    cam : tuple\n",
    "        Grad-Camの画像\n",
    "    heatmap : tuple\n",
    "        ヒートマップ画像\n",
    "    '''\n",
    "\n",
    "    # ----- 1. 入力画像の予測クラスを計算 -----\n",
    "\n",
    "    # 入力のcategory_indexが予想クラス\n",
    "\n",
    "    # ----- 2. 予測クラスのLossを計算 -----\n",
    "    # 分類クラス数\n",
    "    nb_classes = 2\n",
    "\n",
    "    # 入力データxのcategory_indexで指定したインデックス以外を0にする処理の定義\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    x = input_model.output\n",
    "    x = Lambda(target_layer, output_shape=target_category_loss_output_shape)(x)\n",
    "    model = tf.keras.models.Model([input_model.inputs], [input_model.get_layer(layer_name).output, input_model.output])\n",
    "    #model.summary()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    " \n",
    "        # 引数のinput_modelの出力層の後にtarget_layerレイヤーを追加\n",
    "        # modelのpredictをすると予測クラス以外の値は0になる\n",
    "        # 予測クラス以外の値は0なのでsumをとって予測クラスの値のみ抽出\n",
    "        conv_output, preds = model(image)\n",
    "        class_idx = np.argmax(preds[0])\n",
    "        loss = preds[:, class_idx]\n",
    "        # 引数のlayer_nameのレイヤー(最後のconv層)のoutputを取得する\n",
    "        # conv_output = [l for l in model.layers if l.name == layer_name][0].output\n",
    "        # var_list = [conv_output]\n",
    "\n",
    "    # ----- 3. 予測クラスのLossから最後のconv層への逆伝搬(勾配)を計算 -----\n",
    "\n",
    "    # 予想クラスの値から最後のconv層までの勾配を計算する関数を定義\n",
    "    # 定義した関数の\n",
    "    # 入力 : [判定したい画像.shape=(1, 224, 224, 3)]、\n",
    "    # 出力 : [最後のconv層の出力値.shape=(1, 14, 14, 512), 予想クラスの値から最後のconv層までの勾配.shape=(1, 14, 14, 512)]\n",
    "    output =conv_output[0]\n",
    "    grads = tape.gradient(loss,conv_output)[0]\n",
    "\n",
    "    #gradient_function = K.function([model.input], [conv_output, grads[0]])\n",
    "    gate_f = tf.cast(output > 0,'float32')\n",
    "    gate_r = tf.cast(grads > 0,'float32')\n",
    "\n",
    "    # 定義した勾配計算用の関数で計算し、データの次元を整形\n",
    "    # 整形後\n",
    "    # output.shape=(14, 14, 512), grad_val.shape=(14, 14, 512)\n",
    "    #output, grads_val = gradient_function([image])\n",
    "    #output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "    guided_grads = gate_f * gate_r * grads\n",
    "\n",
    "    # ----- 4. 最後のconv層のチャンネル毎に勾配を平均を計算して、各チャンネルの重要度(重み)とする -----\n",
    "\n",
    "    # weights.shape=(512, )\n",
    "    # cam.shape=(14, 14)\n",
    "    # ※疑問点1：camの初期化はzerosでなくて良いのか?\n",
    "    weights = np.mean(guided_grads, axis = (0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "    #cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "    #cam = np.zeros(output.shape[0 : 2], dtype = np.float32)    # 私の自作モデルではこちらを使用\n",
    "\n",
    "    # ----- 5. 最後のconv層の順伝搬の出力にチャンネル毎の重みをかけて、足し合わせて、ReLUを通す -----\n",
    "\n",
    "    # 最後のconv層の順伝搬の出力にチャンネル毎の重みをかけて、足し合わせ\n",
    "    #for i, w in enumerate(weights):\n",
    "    #    cam += w * output[:, :, i]\n",
    "\n",
    "    # 入力画像のサイズにリサイズ(14, 14) → (224, 224)\n",
    "    cam = cv2.resize(cam, (210, 210))\n",
    "    # 負の値を0に置換。処理としてはReLUと同じ。\n",
    "    cam = np.maximum(cam, 0)\n",
    "    # 値を0~1に正規化。\n",
    "    # ※疑問2 : (cam - np.min(cam))/(np.max(cam) - np.min(cam))でなくて良いのか?\n",
    "    heatmap = cam / np.max(cam)\n",
    "    #heatmap = (cam - np.min(cam))/(np.max(cam) - np.min(cam))    # 私の自作モデルではこちらを使用\n",
    "\n",
    "    # ----- 6. 入力画像とheatmapをかける -----\n",
    "\n",
    "    # 入力画像imageの値を0~255に正規化. image.shape=(1, 224, 224, 3) → (224, 224, 3)\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    # ※疑問3 : np.uint8(image / np.max(image))でなくても良いのか?\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    # heatmapの値を0~255にしてカラーマップ化(3チャンネル化)\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    # 入力画像とheatmapの足し合わせ\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    # 値を0~255に正規化\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = InceptionV3(include_top=False, weights='imagenet',input_shape=(210,210,3))\n",
    "#plot_model(base_model,show_shapes=True,to_file='graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(210,210,3))\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(1024,activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(2,activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input,outputs=x)\n",
    "plot_model(model,show_shapes=True,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "    if layer.name.startswith('batch_normalization'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = np.load('washer_OK.npy')\n",
    "x1 = np.load('washer_NG_ponch.npy')\n",
    "x2 = np.load('washer_NG_Scratch.npy')\n",
    "X = np.concatenate([x0[:172],x1,x2],0)\n",
    "X = X.reshape(344,210,210,1)\n",
    "X = np.tile(X,(1,1,1,3))\n",
    "y = np.concatenate([np.zeros((172)),np.ones((172))],0)\n",
    "y = to_categorical(y)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,batch_size=96,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "names = [l.name for l in model.layers]\n",
    "pprint.pprint(names,compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_test.shape[0]):\n",
    "    pre_inp = X_test[i,:,:,:].reshape(-1,210,210,3)\n",
    "    pred = model.predict(pre_inp)\n",
    "    pred_Class = np.argmax(pred)\n",
    "    cam,hm = grad_cam(model,pre_inp,pred_Class,'mixed10')\n",
    "    cv2.imwrite('hm_'+ str(i) +'.png', cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}